Title: StreamRecoder Class in Codecs Module
URL: https://docs.python.org/3/library/codecs.html
Summary: The StreamRecoder class provides a mechanism for two-way data conversion, allowing encoding and decoding operations to be performed transparently on stream data. It utilizes specified encoding and decoding functions along with reader and writer implementations. This is useful for seamlessly transcoding data formats, such as from Latin-1 to UTF-8.
---

_class_ codecs.StreamRecoder(_stream_ , _encode_ , _decode_ , _Reader_ , _Writer_ , _errors ='strict'_)[¶](https://docs.python.org/3/library/codecs.html#codecs.StreamRecoder "Link to this definition")
    
Creates a [`StreamRecoder`](https://docs.python.org/3/library/codecs.html#codecs.StreamRecoder "codecs.StreamRecoder") instance which implements a two-way conversion: _encode_ and _decode_ work on the frontend — the data visible to code calling [`read()`](https://docs.python.org/3/library/codecs.html#codecs.StreamReader.read "codecs.StreamReader.read") and [`write()`](https://docs.python.org/3/library/codecs.html#codecs.StreamWriter.write "codecs.StreamWriter.write"), while _Reader_ and _Writer_ work on the backend — the data in _stream_.
You can use these objects to do transparent transcodings, e.g., from Latin-1 to UTF-8 and back.
The _stream_ argument must be a file-like object.
The _encode_ and _decode_ arguments must adhere to the [`Codec`](https://docs.python.org/3/library/codecs.html#codecs.Codec "codecs.Codec") interface. _Reader_ and _Writer_ must be factory functions or classes providing objects of the [`StreamReader`](https://docs.python.org/3/library/codecs.html#codecs.StreamReader "codecs.StreamReader") and [`StreamWriter`](https://docs.python.org/3/library/codecs.html#codecs.StreamWriter "codecs.StreamWriter") interface respectively.
Error handling is done in the same way as defined for the stream readers and writers.
[`StreamRecoder`](https://docs.python.org/3/library/codecs.html#codecs.StreamRecoder "codecs.StreamRecoder") instances define the combined interfaces of [`StreamReader`](https://docs.python.org/3/library/codecs.html#codecs.StreamReader "codecs.StreamReader") and [`StreamWriter`](https://docs.python.org/3/library/codecs.html#codecs.StreamWriter "codecs.StreamWriter") classes. They inherit all other methods and attributes from the underlying stream.
## Encodings and Unicode[¶](https://docs.python.org/3/library/codecs.html#encodings-and-unicode "Link to this heading")
Strings are stored internally as sequences of code points in range `U+0000`–`U+10FFFF`. (See [**PEP 393**](https://peps.python.org/pep-0393/) for more details about the implementation.) Once a string object is used outside of CPU and memory, endianness and how these arrays are stored as bytes become an issue. As with other codecs, serialising a string into a sequence of bytes is known as _encoding_ , and recreating the string from the sequence of bytes is known as _decoding_. There are a variety of different text serialisation codecs, which are collectivity referred to as [text encodings](https://docs.python.org/3/glossary.html#term-text-encoding).
The simplest text encoding (called `'latin-1'` or `'iso-8859-1'`) maps the code points 0–255 to the bytes `0x0`–`0xff`, which means that a string object that contains code points above `U+00FF` can’t be encoded with this codec. Doing so will raise a [`UnicodeEncodeError`](https://docs.python.org/3/library/exceptions.html#UnicodeEncodeError "UnicodeEncodeError") that looks like the following (although the details of the error message may differ): `UnicodeEncodeError: 'latin-1' codec can't encode character '\u1234' in position 3: ordinal not in range(256)`.
There’s another group of encodings (the so called charmap encodings) that choose a different subset of all Unicode code points and how these code points are mapped to the bytes `0x0`–`0xff`. To see how this is done simply open e.g. `encodings/cp1252.py` (which is an encoding that is used primarily on Windows). There’s a string constant with 256 characters that shows you which character is mapped to which byte value.
All of these encodings can only encode 256 of the 1114112 code points defined in Unicode. A simple and straightforward way that can store each Unicode code point, is to store each code point as four consecutive bytes. There are two possibilities: store the bytes in big endian or in little endian order. These two encodings are called `UTF-32-BE` and `UTF-32-LE` respectively. Their disadvantage is that if e.g. you use `UTF-32-BE` on a little endian machine you will always have to swap bytes on encoding and decoding. `UTF-32` avoids this problem: bytes will always be in natural endianness. When these bytes are read by a CPU with a different endianness, then bytes have to be swapped though. To be able to detect the endianness of a `UTF-16` or `UTF-32` byte sequence, there’s the so called BOM (“Byte Order Mark”). This is the Unicode character `U+FEFF`. This character can be prepended to every `UTF-16` or `UTF-32` byte sequence. The byte swapped version of this character (`0xFFFE`) is an illegal character that may not appear in a Unicode text. So when the first character in a `UTF-16` or `UTF-32` byte sequence appears to be a `U+FFFE` the bytes have to be swapped on decoding. Unfortunately the character `U+FEFF` had a second purpose as a `ZERO WIDTH NO-BREAK SPACE`: a character that has no width and doesn’t allow