Title: Python Tokenize Module Example Output
URL: https://docs.python.org/3/library/tokenize.html
Summary: This chunk demonstrates the output of the Python 'tokenize' module when analyzing a sample Python script. It shows the types and positions of various tokens, such as 'ENCODING', 'NAME', 'OP', and 'NEWLINE', along with a method for displaying exact token type names using the '-e' option.
---

```
$ python-mtokenizehello.py
0,0-0,0:      ENCODING    'utf-8'
1,0-1,3:      NAME      'def'
1,4-1,13:      NAME      'say_hello'
1,13-1,14:     OP       '('
1,14-1,15:     OP       ')'
1,15-1,16:     OP       ':'
1,16-1,17:     NEWLINE    '\n'
2,0-2,4:      INDENT     '  '
2,4-2,9:      NAME      'print'
2,9-2,10:      OP       '('
2,10-2,25:     STRING     '"Hello, World!"'
2,25-2,26:     OP       ')'
2,26-2,27:     NEWLINE    '\n'
3,0-3,1:      NL       '\n'
4,0-4,0:      DEDENT     ''
4,0-4,9:      NAME      'say_hello'
4,9-4,10:      OP       '('
4,10-4,11:     OP       ')'
4,11-4,12:     NEWLINE    '\n'
5,0-5,0:      ENDMARKER   ''

```

The exact token type names can be displayed using the [`-e`](https://docs.python.org/3/library/tokenize.html#cmdoption-tokenize-e) option:
```
$ python-mtokenize-ehello.py
0,0-0,0:      ENCODING    'utf-8'
1,0-1,3:      NAME      'def'
1,4-1,13:      NAME      'say_hello'
1,13-1,14:     LPAR      '('
1,14-1,15:     RPAR      ')'
1,15-1,16:     COLON     ':'
1,16-1,17:     NEWLINE    '\n'
2,0-2,4:      INDENT     '  '
2,4-2,9:      NAME      'print'
2,9-2,10:      LPAR      '('
2,10-2,25:     STRING     '"Hello, World!"'
2,25-2,26:     RPAR      ')'
2,26-2,27:     NEWLINE    '\n'
3,0-3,1:      NL       '\n'
4,0-4,0:      DEDENT     ''
4,0-4,9:      NAME      'say_hello'
4,9-4,10:      LPAR      '('
4,10-4,11:     RPAR      ')'
4,11-4,12:     NEWLINE    '\n'
5,0-5,0:      ENDMARKER   ''

```

Example of tokenizing a file programmatically, reading unicode strings instead of bytes with [`generate_tokens()`](https://docs.python.org/3/library/tokenize.html#tokenize.generate_tokens "tokenize.generate_tokens"):
```
importtokenize
with tokenize.open('hello.py') as f:
  tokens = tokenize.generate_tokens(f.readline)
  for token in tokens:
    print(token)

```

Or reading bytes directly with [`tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize"):
```
importtokenize
with open('hello.py', 'rb') as f:
  tokens = tokenize.tokenize(f.readline)
  for token in tokens:
    print(token)

```

### [Table of Contents](https://docs.python.org/3/contents.html)
  * [`tokenize` — Tokenizer for Python source](https://docs.python.org/3/library/tokenize.html)
    * [Tokenizing Input](https://docs.python.org/3/library/tokenize.html#tokenizing-input)
    * [Command-Line Usage](https://docs.python.org/3/library/tokenize.html#command-line-usage)
    * [Examples](https://docs.python.org/3/library/tokenize.html#examples)


#### Previous topic
[`keyword` — Testing for Python keywords](https://docs.python.org/3/library/keyword.html "previous chapter")
#### Next topic
[`tabnanny` — Detection of ambiguous indentation](https://docs.python.org/3/library/tabnanny.html "next chapter")
### This Page
  * [Report a Bug](https://docs.python.org/3/bugs.html)
  * [Show Source ](https://github.com/python/cpython/blob/main/Doc/library/tokenize.rst)


«
### Navigation
  * [index](https://docs.python.org/3/genindex.html "General Index")
  * [modules](https://docs.python.org/3/py-modindex.html "Python Module Index") |
  * [next](https://docs.python.org/3/library/tabnanny.html "tabnanny — Detection of ambiguous indentation") |
  * [previous](https://docs.python.org/3/library/keyword.html "keyword — Testing for Python keywords") |
  * ![Python logo](https://docs.python.org/3/_static/py.svg)
  * [Python](https://www.python.org/) »
  * EnglishSpanish | españolFrench | françaisItalian | italianoJapanese | 日本語Korean | 한국어Polish | polskiBrazilian Portuguese | Português brasileiroTurkish | TürkçeSimplified Chinese | 简体中文Traditional Chinese | 繁體中文
dev (3.14)3.13.33.123.113.103.93.83.73.63.53.43.33.23.13.02.72.6
  * [3.13.3 Documentation](https://docs.python.org/3/index.html) » 
  * [The Python Standard Library](https://docs.python.org/3/library/index.html) »
  * [Python Language Services](https://docs.python.org/3/library/language.html) »
  * [`tokenize` — Tokenizer for Python source](https://docs.python.org/3/library/tokenize.html)
  * | 
  * Theme  Auto Light Dark |


© [ Copyright ](https://docs.python.org/3/copyright.html) 2001-2025, Python Software Foundation. This page is licensed under the Python Software Foundation License Version 2. Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License. See [History and License](https://docs.python.org/license.html) for more information. The Python Software Foundation is a non-profit corporation. [Please donate.](https://www.python.org/psf/donations/) Last updated on Apr 27, 2025 (05:38 UTC). [Found a bug](https://docs.python.org/bugs.html)? Created using [Sphinx](https://www.sphinx-doc.org/) 8.2.3.