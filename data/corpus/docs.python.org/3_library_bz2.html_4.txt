Title: bz2.decompress() - Decompressing Data in Python
URL: https://docs.python.org/3/library/bz2.html
Summary: The `bz2.decompress()` function decompresses a bytes-like object, allowing for the handling of multiple concatenated compressed streams. For incremental decompression, users should utilize `BZ2Decompressor`. The function's multi-stream support was introduced in version 3.3.
---

bz2.decompress(_data_)[¶](https://docs.python.org/3/library/bz2.html#bz2.decompress "Link to this definition")
    
Decompress _data_ , a [bytes-like object](https://docs.python.org/3/glossary.html#term-bytes-like-object).
If _data_ is the concatenation of multiple compressed streams, decompress all of the streams.
For incremental decompression, use a [`BZ2Decompressor`](https://docs.python.org/3/library/bz2.html#bz2.BZ2Decompressor "bz2.BZ2Decompressor") instead.
Changed in version 3.3: Support for multi-stream inputs was added.
## Examples of usage[¶](https://docs.python.org/3/library/bz2.html#examples-of-usage "Link to this heading")
Below are some examples of typical usage of the [`bz2`](https://docs.python.org/3/library/bz2.html#module-bz2 "bz2: Interfaces for bzip2 compression and decompression.") module.
Using [`compress()`](https://docs.python.org/3/library/bz2.html#bz2.compress "bz2.compress") and [`decompress()`](https://docs.python.org/3/library/bz2.html#bz2.decompress "bz2.decompress") to demonstrate round-trip compression:
>>>```
>>> importbz2
>>> data = b"""\
... Donec rhoncus quis sapien sit amet molestie. Fusce scelerisque vel augue
... nec ullamcorper. Nam rutrum pretium placerat. Aliquam vel tristique lorem,
... sit amet cursus ante. In interdum laoreet mi, sit amet ultrices purus
... pulvinar a. Nam gravida euismod magna, non varius justo tincidunt feugiat.
... Aliquam pharetra lacus non risus vehicula rutrum. Maecenas aliquam leo
... felis. Pellentesque semper nunc sit amet nibh ullamcorper, ac elementum
... dolor luctus. Curabitur lacinia mi ornare consectetur vestibulum."""
>>> c = bz2.compress(data)
>>> len(data) / len(c) # Data compression ratio
1.513595166163142
>>> d = bz2.decompress(c)
>>> data == d # Check equality to original object after round-trip
True

```

Using [`BZ2Compressor`](https://docs.python.org/3/library/bz2.html#bz2.BZ2Compressor "bz2.BZ2Compressor") for incremental compression:
>>>```
>>> importbz2
>>> defgen_data(chunks=10, chunksize=1000):
... """Yield incremental blocks of chunksize bytes."""
...   for _ in range(chunks):
...     yield b"z" * chunksize
...
>>> comp = bz2.BZ2Compressor()
>>> out = b""
>>> for chunk in gen_data():
...   # Provide data to the compressor object
...   out = out + comp.compress(chunk)
...
>>> # Finish the compression process. Call this once you have
>>> # finished providing data to the compressor.
>>> out = out + comp.flush()

```

The example above uses a very “nonrandom” stream of data (a stream of `b"z"` chunks). Random data tends to compress poorly, while ordered, repetitive data usually yields a high compression ratio.
Writing and reading a bzip2-compressed file in binary mode:
>>>```
>>> importbz2
>>> data = b"""\
... Donec rhoncus quis sapien sit amet molestie. Fusce scelerisque vel augue
... nec ullamcorper. Nam rutrum pretium placerat. Aliquam vel tristique lorem,
... sit amet cursus ante. In interdum laoreet mi, sit amet ultrices purus
... pulvinar a. Nam gravida euismod magna, non varius justo tincidunt feugiat.
... Aliquam pharetra lacus non risus vehicula rutrum. Maecenas aliquam leo
... felis. Pellentesque semper nunc sit amet nibh ullamcorper, ac elementum
... dolor luctus. Curabitur lacinia mi ornare consectetur vestibulum."""
>>> with bz2.open("myfile.bz2", "wb") as f:
...   # Write compressed data to file
...   unused = f.write(data)
...
>>> with bz2.open("myfile.bz2", "rb") as f:
...   # Decompress data from file
...   content = f.read()
...
>>> content == data # Check equality to original object after round-trip
True