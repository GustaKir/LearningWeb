Title: Tokenizer for Python Source
URL: https://docs.python.org/3/library/tokenize.html
Summary: This documentation covers the `tokenize` library in Python, which is used for tokenizing Python source code. It includes sections on tokenizing input, command-line usage, and examples.
---

[ ![Python logo](https://docs.python.org/3/_static/py.svg) ](https://www.python.org/) dev (3.14)3.13.33.123.113.103.93.83.73.63.53.43.33.23.13.02.72.6
EnglishSpanish | españolFrench | françaisItalian | italianoJapanese | 日本語Korean | 한국어Polish | polskiBrazilian Portuguese | Português brasileiroTurkish | TürkçeSimplified Chinese | 简体中文Traditional Chinese | 繁體中文
Theme  Auto Light Dark
### [Table of Contents](https://docs.python.org/3/contents.html)
  * [`tokenize` — Tokenizer for Python source](https://docs.python.org/3/library/tokenize.html)
    * [Tokenizing Input](https://docs.python.org/3/library/tokenize.html#tokenizing-input)
    * [Command-Line Usage](https://docs.python.org/3/library/tokenize.html#command-line-usage)
    * [Examples](https://docs.python.org/3/library/tokenize.html#examples)


#### Previous topic
[`keyword` — Testing for Python keywords](https://docs.python.org/3/library/keyword.html "previous chapter")
#### Next topic
[`tabnanny` — Detection of ambiguous indentation](https://docs.python.org/3/library/tabnanny.html "next chapter")
### This Page
  * [Report a Bug](https://docs.python.org/3/bugs.html)
  * [Show Source ](https://github.com/python/cpython/blob/main/Doc/library/tokenize.rst)


### Navigation
  * [index](https://docs.python.org/3/genindex.html "General Index")
  * [modules](https://docs.python.org/3/py-modindex.html "Python Module Index") |
  * [next](https://docs.python.org/3/library/tabnanny.html "tabnanny — Detection of ambiguous indentation") |
  * [previous](https://docs.python.org/3/library/keyword.html "keyword — Testing for Python keywords") |
  * ![Python logo](https://docs.python.org/3/_static/py.svg)
  * [Python](https://www.python.org/) »
  * EnglishSpanish | españolFrench | françaisItalian | italianoJapanese | 日本語Korean | 한국어Polish | polskiBrazilian Portuguese | Português brasileiroTurkish | TürkçeSimplified Chinese | 简体中文Traditional Chinese | 繁體中文
dev (3.14)3.13.33.123.113.103.93.83.73.63.53.43.33.23.13.02.72.6
  * [3.13.3 Documentation](https://docs.python.org/3/index.html) » 
  * [The Python Standard Library](https://docs.python.org/3/library/index.html) »
  * [Python Language Services](https://docs.python.org/3/library/language.html) »
  * [`tokenize` — Tokenizer for Python source](https://docs.python.org/3/library/tokenize.html)
  * | 
  * Theme  Auto Light Dark |


# `tokenize` — Tokenizer for Python source[¶](https://docs.python.org/3/library/tokenize.html#module-tokenize "Link to this heading")
**Source code:** [Lib/tokenize.py](https://github.com/python/cpython/tree/3.13/Lib/tokenize.py)
The [`tokenize`](https://docs.python.org/3/library/tokenize.html#module-tokenize "tokenize: Lexical scanner for Python source code.") module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it useful for implementing “pretty-printers”, including colorizers for on-screen displays.
To simplify token stream handling, all [operator](https://docs.python.org/3/reference/lexical_analysis.html#operators) and [delimiter](https://docs.python.org/3/reference/lexical_analysis.html#delimiters) tokens and [`Ellipsis`](https://docs.python.org/3/library/constants.html#Ellipsis "Ellipsis") are returned using the generic [`OP`](https://docs.python.org/3/library/token.html#token.OP "token.OP") token type. The exact type can be determined by checking the `exact_type` property on the [named tuple](https://docs.python.org/3/glossary.html#term-named-tuple) returned from [`tokenize.tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize").
Warning
Note that the functions in this module are only designed to parse syntactically valid Python code (code that does not raise when parsed using [`ast.parse()`](https://docs.python.org/3/library/ast.html#ast.parse "ast.parse")). The behavior of the functions in this module is **undefined** when providing invalid Python code and it can change at any point.
## Tokenizing Input[¶](https://docs.python.org/3/library/tokenize.html#tokenizing-input "Link to this heading")
The primary entry point is a [generator](https://docs.python.org/3/glossary.html#term-generator):