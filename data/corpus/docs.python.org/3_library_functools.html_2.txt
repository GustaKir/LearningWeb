Title: functools.lru_cache Decorator
URL: https://docs.python.org/3/library/functools.html
Summary: The @functools.lru_cache decorator provides a memoization mechanism for functions, saving the results of up to 'maxsize' recent calls to improve performance on expensive or I/O-bound functions. It is thread-safe, allowing use in concurrent environments, and utilizes a dictionary to cache results based on positional and keyword arguments.
---

```

For sorting examples and a brief sorting tutorial, see [Sorting Techniques](https://docs.python.org/3/howto/sorting.html#sortinghowto).
Added in version 3.2. 

@functools.lru_cache(_user_function_)[¶](https://docs.python.org/3/library/functools.html#functools.lru_cache "Link to this definition")


@functools.lru_cache(_maxsize =128_, _typed =False_)
    
Decorator to wrap a function with a memoizing callable that saves up to the _maxsize_ most recent calls. It can save time when an expensive or I/O bound function is periodically called with the same arguments.
The cache is threadsafe so that the wrapped function can be used in multiple threads. This means that the underlying data structure will remain coherent during concurrent updates.
It is possible for the wrapped function to be called more than once if another thread makes an additional call before the initial call has been completed and cached.
Since a dictionary is used to cache results, the positional and keyword arguments to the function must be [hashable](https://docs.python.org/3/glossary.html#term-hashable).
Distinct argument patterns may be considered to be distinct calls with separate cache entries. For example, `f(a=1, b=2)` and `f(b=2, a=1)` differ in their keyword argument order and may have two separate cache entries.
If _user_function_ is specified, it must be a callable. This allows the _lru_cache_ decorator to be applied directly to a user function, leaving the _maxsize_ at its default value of 128:
```
@lru_cache
defcount_vowels(sentence):
  return sum(sentence.count(vowel) for vowel in 'AEIOUaeiou')

```

If _maxsize_ is set to `None`, the LRU feature is disabled and the cache can grow without bound.
If _typed_ is set to true, function arguments of different types will be cached separately. If _typed_ is false, the implementation will usually regard them as equivalent calls and only cache a single result. (Some types such as _str_ and _int_ may be cached separately even when _typed_ is false.)
Note, type specificity applies only to the function’s immediate arguments rather than their contents. The scalar arguments, `Decimal(42)` and `Fraction(42)` are be treated as distinct calls with distinct results. In contrast, the tuple arguments `('answer', Decimal(42))` and `('answer', Fraction(42))` are treated as equivalent.
The wrapped function is instrumented with a `cache_parameters()` function that returns a new [`dict`](https://docs.python.org/3/library/stdtypes.html#dict "dict") showing the values for _maxsize_ and _typed_. This is for information purposes only. Mutating the values has no effect.
To help measure the effectiveness of the cache and tune the _maxsize_ parameter, the wrapped function is instrumented with a `cache_info()` function that returns a [named tuple](https://docs.python.org/3/glossary.html#term-named-tuple) showing _hits_ , _misses_ , _maxsize_ and _currsize_.
The decorator also provides a `cache_clear()` function for clearing or invalidating the cache.
The original underlying function is accessible through the `__wrapped__` attribute. This is useful for introspection, for bypassing the cache, or for rewrapping the function with a different cache.
The cache keeps references to the arguments and return values until they age out of the cache or until the cache is cleared.
If a method is cached, the `self` instance argument is included in the cache. See [How do I cache method calls?](https://docs.python.org/3/faq/programming.html#faq-cache-method-calls)
An [LRU (least recently used) cache](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_Recently_Used_\(LRU\)) works best when the most recent calls are the best predictors of upcoming calls (for example, the most popular articles on a news server tend to change each day). The cache’s size limit assures that the cache does not grow without bound on long-running processes such as web servers.
In general, the LRU cache should only be used when you want to reuse previously computed values. Accordingly, it doesn’t make sense to cache functions with side-effects, functions that need to create distinct mutable objects on each call (such as generators and async functions), or impure functions such as time() or random().
Example of an LRU cache for static web content:
```
@lru_cache(maxsize=32)
defget_pep(num):
  'Retrieve text of a Python Enhancement Proposal'
  resource = f'https://peps.python.org/pep-{num:04d}'
  try:
    with urllib.request.urlopen(resource) as s:
      return s.read()
  except urllib.error.HTTPError:
    return 'Not Found'
>>> for n in 8, 290, 308, 320, 8, 218, 320, 279, 289, 320, 9991:
...   pep = get_pep(n)
...   print(n, len(pep))
>>> get_pep.cache_info()
CacheInfo(hits=3, misses=8, maxsize=32, currsize=8)

```

Example of efficiently computing [Fibonacci numbers](https://en.wikipedia.org/wiki/Fibonacci_number) using a cache to implement a [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) technique: