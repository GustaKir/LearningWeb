Title: Using the tokenize() Generator in Python
URL: https://docs.python.org/3/library/tokenize.html
Summary: The `tokenize()` generator takes a callable object as an argument, mimicking the `io.IOBase.readline()` method. It yields 5-tuples that contain token information including type, string, starting and ending positions, and the physical line of the token in the source code.
---

tokenize.tokenize(_readline_)[¶](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "Link to this definition")
    
The [`tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize") generator requires one argument, _readline_ , which must be a callable object which provides the same interface as the [`io.IOBase.readline()`](https://docs.python.org/3/library/io.html#io.IOBase.readline "io.IOBase.readline") method of file objects. Each call to the function should return one line of input as bytes.
The generator produces 5-tuples with these members: the token type; the token string; a 2-tuple `(srow, scol)` of ints specifying the row and column where the token begins in the source; a 2-tuple `(erow, ecol)` of ints specifying the row and column where the token ends in the source; and the line on which the token was found. The line passed (the last tuple item) is the _physical_ line. The 5 tuple is returned as a [named tuple](https://docs.python.org/3/glossary.html#term-named-tuple) with the field names: `type string start end line`.
The returned [named tuple](https://docs.python.org/3/glossary.html#term-named-tuple) has an additional property named `exact_type` that contains the exact operator type for [`OP`](https://docs.python.org/3/library/token.html#token.OP "token.OP") tokens. For all other token types `exact_type` equals the named tuple `type` field.
Changed in version 3.1: Added support for named tuples.
Changed in version 3.3: Added support for `exact_type`.
[`tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize") determines the source encoding of the file by looking for a UTF-8 BOM or encoding cookie, according to [**PEP 263**](https://peps.python.org/pep-0263/). 

tokenize.generate_tokens(_readline_)[¶](https://docs.python.org/3/library/tokenize.html#tokenize.generate_tokens "Link to this definition")
    
Tokenize a source reading unicode strings instead of bytes.
Like [`tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize"), the _readline_ argument is a callable returning a single line of input. However, [`generate_tokens()`](https://docs.python.org/3/library/tokenize.html#tokenize.generate_tokens "tokenize.generate_tokens") expects _readline_ to return a str object rather than bytes.
The result is an iterator yielding named tuples, exactly like [`tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize"). It does not yield an [`ENCODING`](https://docs.python.org/3/library/token.html#token.ENCODING "token.ENCODING") token.
All constants from the [`token`](https://docs.python.org/3/library/token.html#module-token "token: Constants representing terminal nodes of the parse tree.") module are also exported from [`tokenize`](https://docs.python.org/3/library/tokenize.html#module-tokenize "tokenize: Lexical scanner for Python source code.").
Another function is provided to reverse the tokenization process. This is useful for creating tools that tokenize a script, modify the token stream, and write back the modified script. 

tokenize.untokenize(_iterable_)[¶](https://docs.python.org/3/library/tokenize.html#tokenize.untokenize "Link to this definition")
    
Converts tokens back into Python source code. The _iterable_ must return sequences with at least two elements, the token type and the token string. Any additional sequence elements are ignored.
The result is guaranteed to tokenize back to match the input so that the conversion is lossless and round-trips are assured. The guarantee applies only to the token type and token string as the spacing between tokens (column positions) may change.
It returns bytes, encoded using the [`ENCODING`](https://docs.python.org/3/library/token.html#token.ENCODING "token.ENCODING") token, which is the first token sequence output by [`tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize"). If there is no encoding token in the input, it returns a str instead.
[`tokenize()`](https://docs.python.org/3/library/tokenize.html#tokenize.tokenize "tokenize.tokenize") needs to detect the encoding of source files it tokenizes. The function it uses to do this is available: