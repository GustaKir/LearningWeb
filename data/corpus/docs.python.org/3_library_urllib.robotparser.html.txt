Title: urllib.robotparser Documentation
URL: https://docs.python.org/3/library/urllib.robotparser.html
Summary: This section provides documentation for the `urllib.robotparser` module in Python, which is used for parsing robots.txt files to determine if a user agent is allowed to access specific URLs. It includes navigation links to previous and next topics, and options for reporting bugs or viewing the source.
---

[ ![Python logo](https://docs.python.org/3/_static/py.svg) ](https://www.python.org/) dev (3.14)3.13.33.123.113.103.93.83.73.63.53.43.33.23.13.02.72.6
EnglishSpanish | españolFrench | françaisItalian | italianoJapanese | 日本語Korean | 한국어Polish | polskiBrazilian Portuguese | Português brasileiroTurkish | TürkçeSimplified Chinese | 简体中文Traditional Chinese | 繁體中文
Theme  Auto Light Dark
#### Previous topic
[`urllib.error` — Exception classes raised by urllib.request](https://docs.python.org/3/library/urllib.error.html "previous chapter")
#### Next topic
[`http` — HTTP modules](https://docs.python.org/3/library/http.html "next chapter")
### This Page
  * [Report a Bug](https://docs.python.org/3/bugs.html)
  * [Show Source ](https://github.com/python/cpython/blob/main/Doc/library/urllib.robotparser.rst)


### Navigation
  * [index](https://docs.python.org/3/genindex.html "General Index")
  * [modules](https://docs.python.org/3/py-modindex.html "Python Module Index") |
  * [next](https://docs.python.org/3/library/http.html "http — HTTP modules") |
  * [previous](https://docs.python.org/3/library/urllib.error.html "urllib.error — Exception classes raised by urllib.request") |
  * ![Python logo](https://docs.python.org/3/_static/py.svg)
  * [Python](https://www.python.org/) »
  * EnglishSpanish | españolFrench | françaisItalian | italianoJapanese | 日本語Korean | 한국어Polish | polskiBrazilian Portuguese | Português brasileiroTurkish | TürkçeSimplified Chinese | 简体中文Traditional Chinese | 繁體中文
dev (3.14)3.13.33.123.113.103.93.83.73.63.53.43.33.23.13.02.72.6
  * [3.13.3 Documentation](https://docs.python.org/3/index.html) » 
  * [The Python Standard Library](https://docs.python.org/3/library/index.html) »
  * [Internet Protocols and Support](https://docs.python.org/3/library/internet.html) »
  * [`urllib.robotparser` — Parser for robots.txt](https://docs.python.org/3/library/urllib.robotparser.html)
  * | 
  * Theme  Auto Light Dark |


# `urllib.robotparser` — Parser for robots.txt[¶](https://docs.python.org/3/library/urllib.robotparser.html#module-urllib.robotparser "Link to this heading")
**Source code:** [Lib/urllib/robotparser.py](https://github.com/python/cpython/tree/3.13/Lib/urllib/robotparser.py)
This module provides a single class, [`RobotFileParser`](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser "urllib.robotparser.RobotFileParser"), which answers questions about whether or not a particular user agent can fetch a URL on the web site that published the `robots.txt` file. For more details on the structure of `robots.txt` files, see <http://www.robotstxt.org/orig.html>. 

_class_ urllib.robotparser.RobotFileParser(_url =''_)[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser "Link to this definition")
    
This class provides methods to read, parse and answer questions about the `robots.txt` file at _url_. 

set_url(_url_)[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser.set_url "Link to this definition")
    
Sets the URL referring to a `robots.txt` file. 

read()[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser.read "Link to this definition")
    
Reads the `robots.txt` URL and feeds it to the parser. 

parse(_lines_)[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser.parse "Link to this definition")
    
Parses the lines argument. 

can_fetch(_useragent_ , _url_)[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser.can_fetch "Link to this definition")
    
Returns `True` if the _useragent_ is allowed to fetch the _url_ according to the rules contained in the parsed `robots.txt` file. 

mtime()[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser.mtime "Link to this definition")
    
Returns the time the `robots.txt` file was last fetched. This is useful for long-running web spiders that need to check for new `robots.txt` files periodically. 

modified()[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser.modified "Link to this definition")
    
Sets the time the `robots.txt` file was last fetched to the current time. 

crawl_delay(_useragent_)[¶](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser.crawl_delay "Link to this definition")
    
Returns the value of the `Crawl-delay` parameter from `robots.txt` for the _useragent_ in question. If there is no such parameter or it doesn’t apply to the _useragent_ specified or the `robots.txt` entry for this parameter has invalid syntax, return `None`.
Added in version 3.6.