Title: Caching in Streamlit: Minimal Example
URL: https://docs.streamlit.io/develop/concepts/architecture/caching
Summary: To cache functions in Streamlit, use decorators like `@st.cache_data`. This ensures that Streamlit checks input parameters and function code; if unchanged, it retrieves cached results instead of re-running the function, improving performance.
---

## [](https://docs.streamlit.io/develop/concepts/architecture/caching#minimal-example)Minimal example
To cache a function in Streamlit, you must decorate it with one of two decorators (`st.cache_data` or `st.cache_resource`):
`@st.cache_data def long_running_function(param1, param2): return … `
In this example, decorating `long_running_function` with `@st.cache_data` tells Streamlit that whenever the function is called, it checks two things:
  1. The values of the input parameters (in this case, `param1` and `param2`).
  2. The code inside the function.


If this is the first time Streamlit sees these parameter values and function code, it runs the function and stores the return value in a cache. The next time the function is called with the same parameters and code (e.g., when a user interacts with the app), Streamlit will skip executing the function altogether and return the cached value instead. During development, the cache updates automatically as the function code changes, ensuring that the latest changes are reflected in the cache.
As mentioned, there are two caching decorators:
  * `st.cache_data` is the recommended way to cache computations that return data: loading a DataFrame from CSV, transforming a NumPy array, querying an API, or any other function that returns a serializable data object (str, int, float, DataFrame, array, list, …). It creates a new copy of the data at each function call, making it safe against [mutations and race conditions](https://docs.streamlit.io/develop/concepts/architecture/caching#mutation-and-concurrency-issues). The behavior of `st.cache_data` is what you want in most cases – so if you're unsure, start with `st.cache_data` and see if it works!
  * `st.cache_resource` is the recommended way to cache global resources like ML models or database connections – unserializable objects that you don't want to load multiple times. Using it, you can share these resources across all reruns and sessions of an app without copying or duplication. Note that any mutations to the cached return value directly mutate the object in the cache (more details below).