Title: Caching in Streamlit
URL: https://docs.streamlit.io/develop/concepts/architecture/caching
Summary: This section explains how Streamlit's caching system works for both commands and UI components. It describes the caching of `st.` commands to maintain their state across runs and illustrates how to cache parts of the UI, including the use of interactive input widgets within cached functions. To enable experimental features for widget replay, the `experimental_allow_widgets` parameter must be set.
---

But what happens on subsequent runs? It still shows up! Streamlit realizes that there is an `st.` command inside the cached function, saves it during the first run, and replays it on subsequent runs. Replaying static elements works for both caching decorators.
You can also use this functionality to cache entire parts of your UI:
`@st.cache_data def show_data():   st.header("Data analysis")   data = api.get(...)   st.success("Fetched data from API!")   st.write("Here is a plot of the data:")   st.line_chart(data)   st.write("And here is the raw data:")   st.dataframe(data) `
#### [](https://docs.streamlit.io/develop/concepts/architecture/caching#input-widgets)Input widgets
You can also use [interactive input widgets](https://docs.streamlit.io/develop/api-reference/widgets) like `st.slider` or `st.text_input` in cached functions. Widget replay is an experimental feature at the moment. To enable it, you need to set the `experimental_allow_widgets` parameter:
`@st.cache_data(experimental_allow_widgets=True) # ðŸ‘ˆ Set the parameter def get_data():   num_rows = st.slider("Number of rows to get") # ðŸ‘ˆ Add a slider   data = api.get(..., num_rows) return data `
Streamlit treats the slider like an additional input parameter to the cached function. If you change the slider position, Streamlit will see if it has already cached the function for this slider value. If yes, it will return the cached value. If not, it will rerun the function using the new slider value.
Using widgets in cached functions is extremely powerful because it lets you cache entire parts of your app. But it can be dangerous! Since Streamlit treats the widget value as an additional input parameter, it can easily lead to excessive memory usage. Imagine your cached function has five sliders and returns a 100 MB DataFrame. Then we'll add 100 MB to the cache for _every permutation_ of these five slider values â€“ even if the sliders do not influence the returned data! These additions can make your cache explode very quickly. Please be aware of this limitation if you use widgets in cached functions. We recommend using this feature only for isolated parts of your UI where the widgets directly influence the cached return value.
_priority_high_
#### Warning
Support for widgets in cached functions is experimental. We may change or remove it anytime without warning. Please use it with care!
_push_pin_
#### Note
Two widgets are currently not supported in cached functions: `st.file_uploader` and `st.camera_input`. We may support them in the future. Feel free to [open a GitHub issue](https://github.com/streamlit/streamlit/issues) if you need them!
### [](https://docs.streamlit.io/develop/concepts/architecture/caching#dealing-with-large-data)Dealing with large data
As we explained, you should cache data objects with `st.cache_data`. But this can be slow for extremely large data, e.g., DataFrames or arrays with >100 million rows. That's because of the [copying behavior](https://docs.streamlit.io/develop/concepts/architecture/caching#copying-behavior) of `st.cache_data`: on the first run, it serializes the return value to bytes and deserializes it on subsequent runs. Both operations take time.
If you're dealing with extremely large data, it can make sense to use `st.cache_resource` instead. It does not create a copy of the return value via serialization/deserialization and is almost instant. But watch out: any mutation to the function's return value (such as dropping a column from a DataFrame or setting a value in an array) directly manipulates the object in the cache. You must ensure this doesn't corrupt your data or lead to crashes. See the section on [Mutation and concurrency issues](https://docs.streamlit.io/develop/concepts/architecture/caching#mutation-and-concurrency-issues) below.
When benchmarking `st.cache_data` on pandas DataFrames with four columns, we found that it becomes slow when going beyond 100 million rows. The table shows runtimes for both caching decorators at different numbers of rows (all with four columns):
| | 10M rows| 50M rows| 100M rows| 200M rows  
---|---|---|---|---|---  
st.cache_data| First run*| 0.4 s| 3 s| 14 s| 28 s  
| Subsequent runs| 0.2 s| 1 s| 2 s| 7 s  
st.cache_resource| First run*| 0.01 s| 0.1 s| 0.2 s| 1 s  
| Subsequent runs| 0 s| 0 s| 0 s| 0 s  
---  
_*For the first run, the table only shows the overhead time of using the caching decorator. It does not include the runtime of the cached function itself._  
### [](https://docs.streamlit.io/develop/concepts/architecture/caching#mutation-and-concurrency-issues)Mutation and concurrency issues
In the sections above, we talked a lot about issues when mutating return objects of cached functions. This topic is complicated! But it's central to understanding the behavior differences between `st.cache_data` and `st.cache_resource`.